## Multivariate autoregressive model example

In this case study we analyze multivariate time series using generic linear stochastic dynamical systems models. 
We will use these models to analyze multispecies time series and extract information on time series interactions and forcing.

The model we will analyze is given as 

$$ \mathbf{x}(t+1) = \mathbf{\Phi} \mathbf{x}(t) + \mathbf{e}(t) $$

where $\mathbf{x}(t)$ is a multivariate time series at time $t$, $\mathbf{\Phi}$ is the linear dynamics matrix, and $\mathbf{e}(t)$ is a realization of a stochastic variable, assumed here multivariate normal with mean vector zero and covariance matrix, $\mathcal{N}(\mathbf{\mu},\mathbf{\Sigma})$.

In general, we can think of this model as a first order approximation to the dynamics of a more general multivariate dynamical system. 
In this context, the matrix $\mathbf{\Phi}$ would represent the Jacobian matrix, containing information on the discretized partial derivatives of each state variable with respect to each of the others. 
The matrix $\mathbf{\Sigma}$ contains information on the magnitude of stochastic forcing as well as correlations among the variables responses.

To explore this model we will first simulate some data


```{r}
library(MASS)     #package for the multi-variate normal distribution
library(rstan)
options(mc.cores = parallel::detectCores())
```

The procedure below uses some linear algebra (specifically the $QR$ decomposition) to generate a random matrix that will stable, i.e. that simulated time series won't blow-up.
We do so by making the eigenvalues of the random matrix lie between 0 and 1.


```{r}
p     <- 3

SIGMA  <- diag(runif(p))   #generate random numbers

eig    <- runif(p,0,1)
Z      <- matrix(ncol=p, rnorm(p^2))
decomp <- qr(Z)
Q      <- qr.Q(decomp)
R      <- qr.R(decomp)
d      <- diag(R)
ph     <- d / abs(d)
O      <- Q %*% diag(ph)
PHI    <- t(O) %*% diag(eig) %*% O
```

We simulate time series given the specified parameters. 
Notice we are drawing a multivariate normal realization every time step with mean zero and covariance matrix `SIGMA`.


```{r}
T  <- 200
y0 <- rnorm(p)
Y     <- matrix(NA,p,T)
Y[,1] <- y0

for(t in 2:T){
    Y[,t] <- PHI%*%Y[,t-1] + mvrnorm(1,rep(0,3),SIGMA)}
```

Plot the system


```{r}
options(repr.plot.width=6, repr.plot.height=4)
matplot(t(Y),type='l')
```

The Stan code below implements the first order multivariate model.
As per usual, we pass Stan the data and its dimensions in the `data` block.
In the `parameters` block, we define the linear dynamics matrix `PHI` which has as many rows and columns as there are time series.
In this case we assume the covariance matrix is of the form

$$ \mathbf{\Sigma} = \begin{bmatrix} \sigma_{1,1} & 0 & \cdots & 0\\
                                    0 & \sigma_{2,2} & \ddots & \vdots \\
                                    \vdots & \ddots & \ddots & \vdots \\
                                    0 & \cdots & \cdots & \sigma_{p,p} \end{bmatrix}   $$
                                    
which means that each species in the mulispecies time series responds to the environment independently.


```{r}
mod_code <- "data {
	int T;         //length of time series
	int p;         //number of variables
	matrix[p,T] Y; //matrix of observations; variables are rows; time is columns
}
parameters{
	matrix[p,p] PHI;     //dynamics matrix
	vector<lower=1E-15>[p] sigma;     //variances of stochastic forcing
	vector[p] init;      //mean of initial conditions as parameter vector
}
model{
	Y[,1] ~ normal(init, sigma);           //distribution of the initial conditions
	for(i in 2:T){
		Y[,i] ~ normal(PHI*Y[,i-1],sigma); //conditional predictive distribution
	}
}"
```

Compile the model

```{r}
mod <- stan_model(model_code=mod_code)
```

Organize the data for Stan

```{r}
data <- list(p=p,T=T,Y=Y)
```

Perform mcmc


```{r}
mcmc <- sampling(mod,data=data,iter=2000,warmup=1000,open_progress=FALSE)
```


```{r}
mcmc
```




### Investigating the stochastic forcing matrix
Here we relax the assumption of independent forcing and specify a covariance matrix with non zero off diagonal elements

$$ \mathbf{\Sigma} = \begin{bmatrix} \sigma_{1,1} & \sigma_{1,2} & \cdots & \sigma_{1,p}\\
                                    \sigma_{2,1} & \sigma_{2,2} & \ddots & \vdots \\
                                    \vdots & \ddots & \ddots & \vdots \\
                                    \sigma_{p,1} & \cdots & \cdots & \sigma_{p,p} \end{bmatrix}   $$

Generate a random matrix that is a proper covariance matrix, i.e. it is positive semi-definite


```{r}
A      <- matrix(runif(p^2)*2-1, ncol=p)   #generate random numbers
SIGMA2 <- t(A) %*% A   
SIGMA2
```



Simulate synthetic time series with the covariance matrix above


```{r}
T      <- 200
y20    <- rnorm(p)
Y2     <- matrix(NA,p,T)
Y2[,1] <- y20

for(t in 2:T){
    Y2[,t] <- PHI%*%Y2[,t-1] + mvrnorm(1,rep(0,3),SIGMA2)}
```

Plot the time series


```{r}
options(repr.plot.width=6, repr.plot.height=4)
matplot(t(Y2),type='l')
```



Package the data


```{r}
data2 <- list(p=p,T=T,Y=Y2)
```

We estimate the full covariance in the Stan code below.
We make use of the very handy parameter type `cov_matrix` which maintains the positive semi-definite requirement throughout the mcmc iterations.


```{r}
mod_code_cov <- "data {
	int T;         //length of time series
	int p;         //number of variables
	matrix[p,T] Y; //matrix of observations; variables are rows; time is columns
}
parameters{
	matrix[p,p] PHI;     //dynamics matrix
	cov_matrix[p] SIGMA; //co-variance matrix of stochastic forcing
	vector[p] init;      //mean of initial conditions as parameter vector
}
model{
	Y[,1] ~ multi_normal(init, SIGMA);           //distribution of the initial conditions
	for(i in 2:T){
		Y[,i] ~ multi_normal(PHI*Y[,i-1],SIGMA); //conditional predictive distribution
	}
}"
```

Compile the model

```{r}
mod_cov <- stan_model(model_code=mod_code_cov)
```

Perform mcmc

```{r}
mcmc_cov <- sampling(mod_cov,data=data2,chains=4,open_progress=FALSE)
```

```{r}
mcmc_cov
```

### Incorporating informative priors on the structure of the dynamics

Here we will impose informative priors on particular elements of the $\mathbf{\Phi}$

The Stan code below imposes the following structure on $\mathbf{\Phi}$:

$$ \mathbf{\Phi} = \begin{bmatrix} \phi_{1,1} & \phi_{1,2} & 0 \\ 
                                \phi_{2,1} & \phi_{2,2} & \phi_{2,3}\\
                                0       & \phi_{3,2} & \phi_{3,3} \end{bmatrix} $$
                                
The prior assumption is that $\phi_{1,3}$ and $\phi_{3,1}$ have means of zero and a very small standard deviation of $10^{-3}$

```{r}
mod_code_D_struc <- "data {
	int T;         //length of time series
	int p;         //number of variables
	matrix[p,T] Y; //matrix of observations; variables are rows; time is columns
}
parameters{
	matrix[p,p] PHI;               //dynamics matrix
	vector<lower=1E-15>[p] sigma;  //variances of stochastic forcing
	vector[p] init;                //mean of initial conditions as parameter vector
}
model{
    PHI[1,3] ~ normal(0,1E-3);
    PHI[3,1] ~ normal(0,1E-3);

	Y[,1] ~ normal(init, sigma);           //distribution of the initial conditions
	for(i in 2:T){
		Y[,i] ~ normal(PHI*Y[,i-1],sigma); //conditional predictive distribution
	}
}"
```


```{r}
mod_struc <- stan_model(model_code=mod_code_D_struc)
```


```{r}
mcmc_struc <- sampling(mod_struc,data=data,chains=4,open_progress=FALSE)
```


```{r}
mcmc_struc
```


